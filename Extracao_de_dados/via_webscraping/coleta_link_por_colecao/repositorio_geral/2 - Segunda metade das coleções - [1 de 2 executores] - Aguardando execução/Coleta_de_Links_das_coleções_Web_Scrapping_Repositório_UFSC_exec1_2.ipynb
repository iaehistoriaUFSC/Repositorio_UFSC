{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iaehistoriaUFSC/Repositorio_UFSC/blob/main/Extracao_de_dados/via_webscraping/coleta_link_por_colecao/repositorio_geral/2%20-%20Segunda%20metade%20das%20colec%CC%A7o%CC%83es%20-%20%5B1%20de%202%20executores%5D%20-%20Aguardando%20execuc%CC%A7a%CC%83o/Coleta_de_Links_das_cole%C3%A7%C3%B5es_Web_Scrapping_Reposit%C3%B3rio_UFSC_exec1_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preparação do ambiente"
      ],
      "metadata": {
        "id": "sUa0av70UapY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Execução 1** - baixando e importanto bibliotecas utilizadas ao decorrer da execução do programa"
      ],
      "metadata": {
        "id": "fWCPlMKKUeXB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYt5CpmzUL1I"
      },
      "outputs": [],
      "source": [
        "# Instalando as bibliotecas que serão utilizadas\n",
        "try:\n",
        "  !pip install requests\n",
        "  !pip install beautifulsoup4\n",
        "  !pip install pandas\n",
        "  !pip install regex\n",
        "  !pip install urllib3\n",
        "except: # Caso ocorra algum erro ao instalar os pacotes/bibliotecas seremos notificados com uma mensagem na tela de output\n",
        "  print('\\nErro ao instalar pacotes/bibliotecas.')\n",
        "  print('Descrição do erro --> '+str(sys.exc_info()[0].__name__)+\": \"+str(sys.exc_info()[1])+'\\n')\n",
        "else:\n",
        "  from google.colab import drive\n",
        "  from google.colab import output\n",
        "  import requests\n",
        "  from bs4 import BeautifulSoup\n",
        "  import pandas as pd\n",
        "  import time\n",
        "  import re\n",
        "  from urllib import request\n",
        "  import os\n",
        "  import sys\n",
        "  execucao = True # Variável que armazenará o sucesso no download das bibliotecas\n",
        "  output.clear()\n",
        "  print('='*100)\n",
        "  print('Bibliotecas carregadas!\\nPode executar o código abaixo.')\n",
        "  print('='*100)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Execução 2** - Conectando ambiente do Google Colab ao Google Drive"
      ],
      "metadata": {
        "id": "z7iWVlZ5Uz6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  drive.mount('/content/drive')\n",
        "except:\n",
        "  print('\\nErro ao \"sincronizar\" Google Drive com este ambiente do Google Colab.\\nExecute a célula novamente ou tente \"montar\" o Drive manualmente no botão de \"Montar Drive\" no menu lateral esquerdo.')\n",
        "  print('Descrição do erro --> '+str(sys.exc_info()[0].__name__)+\":\",str(sys.exc_info()[1])+'\\n')\n",
        "  execucao = False\n",
        "else:\n",
        "  print('='*100)\n",
        "  print('Drive conectado com sucesso.\\nPode prosseguir na execução das próximas células.')\n",
        "  print('='*100)"
      ],
      "metadata": {
        "id": "Yi8GVQsGU0JU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Execução 3** - Definindo variáveis e funções que serão utilizadas ao decorrer do programa"
      ],
      "metadata": {
        "id": "9iaZhv0KU6B8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if execucao:\n",
        "\n",
        "\n",
        "  headers = {'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \\\n",
        "                (KHTML, like Gecko) Chrome/104.0.0.0 Safari/537.36\"}\n",
        "  timeout = 30\n",
        "\n",
        "  lista_de_falhas = {'Avisos':[],'Erros':[]}\n",
        "\n",
        "\n",
        "  try:\n",
        "    def CriaDiretorio(caminho : str):\n",
        "      try:\n",
        "        os.makedirs(caminho)\n",
        "        time.sleep(1)\n",
        "        if os.path.isdir(caminho):\n",
        "          print(f'Pasta {caminho} criada com sucesso.')\n",
        "          return True\n",
        "        else:\n",
        "          print(f'Tentando criar pasta {caminho} novamente.')\n",
        "          time.sleep(3)\n",
        "          if os.path.isdir(caminho):\n",
        "            print(f'Pasta {caminho} criada com sucesso.')\n",
        "            return True\n",
        "          else:\n",
        "            print(f'Pasta {caminho} não foi carregada corretamente.')\n",
        "            return False\n",
        "      except:\n",
        "        print(f'Erro ao criar diretório {caminho}.\\nDescrição do erro --> '+str(sys.exc_info()[0].__name__)+\": \"+str(sys.exc_info()[1])+'\\n')\n",
        "        return False\n",
        "\n",
        "    def ColetaHTML(link_pagina : str):\n",
        "      try:\n",
        "        site = requests.get(link_pagina,headers=headers,timeout=timeout)\n",
        "        if site.status_code == 200:\n",
        "          soup = BeautifulSoup(site.content, 'html.parser')\n",
        "          return True, soup\n",
        "        else:\n",
        "          print(f'Problema ao coletar HTML da página {link_pagina}.\\nsite.status_code != 200')\n",
        "          return False, ''\n",
        "      except:\n",
        "        print(f'Problema ao coletar HTML da página {link_pagina}.\\nDescrição do erro --> '+str(sys.exc_info()[0].__name__)+\": \"+str(sys.exc_info()[1])+'.')\n",
        "        return False, ''\n",
        "\n",
        "\n",
        "    def ColetaNomesLinksColecoesDoHTML(soup):\n",
        "      lista_nomes_colecoes = []\n",
        "      lista_links_colecoes = []\n",
        "\n",
        "      campo_colecoes = soup.find('div', class_='ds-static-div secondary')\n",
        "      if campo_colecoes != None:\n",
        "        colecoes = campo_colecoes.find_all('li')\n",
        "        if len(colecoes) == 0:\n",
        "          print('\\nNão foi possível encontrar os elementos das coleções corretamente.')\n",
        "          lista_de_falhas['Erros'].append('Não foi possível encontrar os elementos das coleções corretamente.')\n",
        "          return False, '', ''\n",
        "        else:\n",
        "          print('Quantidade de coleções avistadas:',len(colecoes))\n",
        "\n",
        "          tamanho_total = len(colecoes)\n",
        "          metade_tamanho_total = int(tamanho_total/2)\n",
        "\n",
        "          for indice, colecao in enumerate(colecoes[metade_tamanho_total:]):\n",
        "            nome_e_link_colecao = colecao.find('a', href=re.compile('/handle'))\n",
        "            try:\n",
        "              nome_colecao = nome_e_link_colecao.text\n",
        "\n",
        "              lista_nomes_colecoes.append(nome_colecao)\n",
        "\n",
        "              link_colecao = nome_e_link_colecao['href']\n",
        "              link_colecao = 'https://repositorio.ufsc.br'+link_colecao\n",
        "\n",
        "              lista_links_colecoes.append(link_colecao)\n",
        "            except:\n",
        "              print(f'\\nErro ao encontrar o nome e link da coleção número {indice} da lista.\\nDescrição do erro --> '+str(sys.exc_info()[0].__name__)+\":\",str(sys.exc_info()[1])+'\\n')\n",
        "              lista_de_falhas['Avisos'].append(f'Erro ao encontrar o nome e link da coleção número {indice} da lista.\\nDescrição do erro --> '+str(sys.exc_info()[0].__name__)+\": \"+str(sys.exc_info()[1])+'\\n')\n",
        "            # break # Só vai pegar a primeira coleção\n",
        "\n",
        "            # Limitando número de coleções que serão visitadas\n",
        "            # limite_colecoes = 3\n",
        "            # if indice >= limite_colecoes: # quebra da iteração\n",
        "            #   print(f'\\nLIMITE DE COLEÇÕES ({limite_colecoes+1}) ATINGIDO.')\n",
        "            #   break\n",
        "\n",
        "          if lista_nomes_colecoes and lista_links_colecoes:\n",
        "            return True, lista_nomes_colecoes, lista_links_colecoes\n",
        "          else:\n",
        "            return False, [], []\n",
        "\n",
        "      else:\n",
        "        print('Campo de coleções não identificado corretamente na página. \"campo_colecoes == None\"!')\n",
        "        return False,'',''\n",
        "\n",
        "    def ColetaLinksTrabalhosDoHTML(soup_colecao,link_principal_colecao):\n",
        "      lista_links_trabalhos = []\n",
        "      numero_paginas = soup_colecao.find('div', attrs={\"class\":'pagination-masked top'})\n",
        "      if numero_paginas == None:\n",
        "        print('\\nNão foi possível achar número de páginas corretamente.\\n(numero_paginas == None).')\n",
        "        lista_de_falhas[\"Avisos\"].append(f'Não foi possível achar número de páginas corretamente da coleção {dic[\"Colecoes\"][i]} --> (numero_paginas == None).')\n",
        "        return False, []\n",
        "      else:\n",
        "        paginas_links = numero_paginas.find('ul',class_='pagination-links')\n",
        "        if paginas_links == None:\n",
        "          print('\\nNão foi possível achar número de páginas corretamente.\\n(paginas_links == None)')\n",
        "          lista_de_falhas[\"Avisos\"].append(f'Não foi possível achar número de páginas corretamente da coleção {dic[\"Colecoes\"][i]} --> (paginas_links == None).')\n",
        "          return False, []\n",
        "        else:\n",
        "          links_pagina = paginas_links.find_all('li')\n",
        "          if links_pagina == None:\n",
        "            print('\\nNão foi possível achar número de páginas corretamente.\\n(links_pagina == None)')\n",
        "            lista_de_falhas[\"Avisos\"].append(f'Não foi possível achar número de páginas corretamente da coleção {dic[\"Colecoes\"][i]} --> (links_pagina == None).')\n",
        "            return False, []\n",
        "          else:\n",
        "            try:\n",
        "              numero_paginas = int(links_pagina[len(links_pagina)-1].find('a').text)\n",
        "            except:\n",
        "              print('\\nNúmero de páginas não pode ser convertido para inteiro, logo foi setado como 1.\\nDescrição do erro --> '+str(sys.exc_info()[0].__name__)+\": \"+str(sys.exc_info()[1])+'\\n')\n",
        "              numero_paginas = 1\n",
        "              lista_de_falhas[\"Avisos\"].append(f'Número de páginas da coleção {dic[\"Colecoes\"][i]} não pode ser convertido para inteiro, logo foi setado como 1\\nDescrição do erro --> '+str(sys.exc_info()[0].__name__)+\": \"+str(sys.exc_info()[1])+'\\n')\n",
        "            print('\\nNúmero total de páginas:',numero_paginas)\n",
        "\n",
        "\n",
        "            links = soup_colecao.find_all('li', class_=re.compile('ds-artifact-item clearfix'))\n",
        "            if len(links) == 0:\n",
        "              print('\\nNão foi possível encontrar corretamente os links para os trabalhos.')\n",
        "              lista_de_falhas[\"Avisos\"].append(f'Não foi possível encontrar/captar corretamente os links para os trabalhos da coleção {dic[\"Colecoes\"][i]}.')\n",
        "              return False, []\n",
        "            else:\n",
        "              pagina = 1\n",
        "\n",
        "              while(pagina <= numero_paginas):\n",
        "                print('\\nIndo para página:',pagina)\n",
        "                link_p_trabalhos = link_principal_colecao + f'/discover?rpp=50&etal=0&group_by=none&page={pagina}'\n",
        "      #                                  MUDAR O RPP = 50 PARA RPP = 100  QUANDO FOR FAZER NO REPOSITÓRIO TODO (falar com TI da BU antes) !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "                status_conteudo_HTML_links_trabalho, soup_links_trabalho = ColetaHTML(link_p_trabalhos)\n",
        "                if status_conteudo_HTML_links_trabalho:\n",
        "                  links = soup_links_trabalho.find_all('li', class_=re.compile('ds-artifact-item clearfix'))\n",
        "                  if len(links) == 0:\n",
        "                    print(f'\\nNão foi possível encontrar corretamente os links dos resultados da coleção {dic[\"Colecoes\"][i]} no link: \"{link}\".')\n",
        "                    lista_de_falhas['Avisos'].append(f'Não foi possível encontrar/coletar corretamente os links dos resultados da coleção {dic[\"Colecoes\"][i]} no link: \"{link}\".')\n",
        "                  else:\n",
        "                    for links_trabalho in links:\n",
        "                      link_trabalho = links_trabalho.find('a', href=re.compile('/handle'))\n",
        "                      if link_trabalho != None:\n",
        "                        link_trabalho = link_trabalho['href']\n",
        "                        link_trabalho = 'https://repositorio.ufsc.br'+link_trabalho\n",
        "                        lista_links_trabalhos.append(link_trabalho)\n",
        "                      else:\n",
        "                        print(f'\\nLink do trabalho não foi encontrado corretamente. Coleção: {dic[\"Colecoes\"][i]}, link: \"{link}\".')\n",
        "                        lista_de_falhas['Avisos'].append(f'Link do trabalho não foi encontrado corretamente. Coleção: {dic[\"Colecoes\"][i]}, link: \"{link}\".')\n",
        "\n",
        "                  # # Limitando a busca (primeiramente não vamos passar por todos os resultados)\n",
        "                  # limite_pagina = 2\n",
        "                  # if pagina >= limite_pagina:\n",
        "                  #   print(f'\\nLIMITE DE PÁGINAS ({limite_pagina}) ALCANÇADO.')\n",
        "                  #   break\n",
        "                  # else:\n",
        "                  #   pagina += 1\n",
        "                  pagina += 1\n",
        "              return True, lista_links_trabalhos\n",
        "\n",
        "  except:\n",
        "    print('Erro ao criar funções.\\nDescrição do erro --> '+str(sys.exc_info()[0].__name__)+\": \"+str(sys.exc_info()[1])+'\\n')\n",
        "  else:\n",
        "    print('='*100)\n",
        "    print('Variáveis para requisições e funções de criação de diretório criadas com sucesso.\\nPode dar continuidade na execução das células abaixo.')\n",
        "    print('='*100)\n",
        "else:\n",
        "  print('\\nFalha no carregamento das bibliotecas ou sincronização com Drive. Tente executar as primeiras células de códigos novamente.')"
      ],
      "metadata": {
        "id": "EOGCBaCwU5ia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Execução das ações, de fato"
      ],
      "metadata": {
        "id": "oxr2uTYLVFIl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Execução 4** - Criação de pasta \"Coleções Repositorio UFSC\" e sub-pasta \"Links dos trabalhos - Repositorio UFSC\" para armazenamento dos primeiros dados (links das coleções)"
      ],
      "metadata": {
        "id": "LKSAMCEuU_Sh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if execucao:\n",
        "  try:\n",
        "    # CriaDiretorio(\"/content/drive/MyDrive/Programa - Repositório Institucional UFSC/Extração de Dados/via Web Scraping/Resultados/Coleções Repositorio UFSC/Links dos trabalhos - Repositorio UFSC\")\n",
        "    CriaDiretorio(\"/content/drive/MyDrive/Programa - Repositório Institucional UFSC/Extração de Dados/via Web Scraping/Otimizações/Resultados/Coleções Repositorio UFSC/Links dos trabalhos - Repositorio UFSC\")\n",
        "  except:\n",
        "    print('\\nErro ao criar pasta no Drive com o código do Google Colab.\\nDescrição do erro --> '+str(sys.exc_info()[0].__name__)+\":\",str(sys.exc_info()[1])+'\\n')\n",
        "    execucao = False\n",
        "  else:\n",
        "    print('='*100)\n",
        "    print('Pastas de trabalhos \"Coleções Repositorio UFSC\" e \"Links dos trabalhos - Repositorio UFSC\" criadas com sucesso!\\nPode continuar e executar os códigos abaixo.')\n",
        "    print('='*100)\n",
        "else:\n",
        "  print('\\nFalha no carregamento das bibliotecas. Tente executar a primeira célula de códigos novamente.\\n')"
      ],
      "metadata": {
        "id": "GpRRqMMQU-mG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Execução 5** - Preenchimento de dados (links) das coleções e dos trabalhos de cada coleção"
      ],
      "metadata": {
        "id": "_oEngsfmVPSZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if execucao:\n",
        "\n",
        "  start = time.time()\n",
        "\n",
        "\n",
        "  link = 'https://repositorio.ufsc.br/handle/123456789/74645'\n",
        "\n",
        "\n",
        "  status_conteudo_HTML, soup_repositorio = ColetaHTML(link)\n",
        "  if status_conteudo_HTML:\n",
        "    numero_total_de_trabalhos = 0\n",
        "    dic = {'Colecoes':[],'Links colecoes':[]}\n",
        "    lista_links_trabalhos = []\n",
        "\n",
        "    status_info_HTML,lista_nomes_colecoes,lista_links_colecoes = ColetaNomesLinksColecoesDoHTML(soup_repositorio)\n",
        "    if status_info_HTML:\n",
        "      dic['Colecoes'] = lista_nomes_colecoes\n",
        "      dic['Links colecoes'] = lista_links_colecoes\n",
        "      print('\\nNúmero de coleções encontradas:',len(dic['Links colecoes']))\n",
        "      num_colecao = 0\n",
        "\n",
        "\n",
        "      for i in range(len(dic['Links colecoes'])):\n",
        "        dic_trabalho = {'Links trabalhos':[]}\n",
        "        num_colecao +=1\n",
        "        link_principal_colecao = dic['Links colecoes'][i]\n",
        "\n",
        "        link_colecao_atual = link_principal_colecao + '/discover?rpp=50&etal=0&group_by=none&page=1'\n",
        "    #                                  MUDAR O RPP = 50 PARA RPP = 100  QUANDO FOR FAZER NO REPOSITÓRIO TODO (falar com TI da BU antes) !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "\n",
        "        status_conteudo_HTML, soup_colecao = ColetaHTML(link_colecao_atual)\n",
        "        if status_conteudo_HTML:\n",
        "          status_links_trabalhos, lista_links_trabalhos = ColetaLinksTrabalhosDoHTML(soup_colecao,link_principal_colecao)\n",
        "          if status_links_trabalhos:\n",
        "            dic_trabalho['Links trabalhos'] = lista_links_trabalhos\n",
        "            numero_total_de_trabalhos += len(lista_links_trabalhos)\n",
        "            print('\\nNúmero de links coletados:',len(dic_trabalho[\"Links trabalhos\"]))\n",
        "\n",
        "            try:\n",
        "              df = pd.DataFrame(dic_trabalho)\n",
        "            except:\n",
        "              print('Problemas ao transformar o dicionário \"dic_trabalho\" num dataframe pandas.')\n",
        "            else:\n",
        "              if '/' in dic[\"Colecoes\"][i]:\n",
        "                index = dic[\"Colecoes\"][i].find('/')\n",
        "                dic[\"Colecoes\"][i] = dic[\"Colecoes\"][i][:index]+'_'+dic[\"Colecoes\"][i][index+1:]\n",
        "              if ':' in dic[\"Colecoes\"][i]:\n",
        "                index = dic[\"Colecoes\"][i].find(':')\n",
        "                dic[\"Colecoes\"][i] = dic[\"Colecoes\"][i][:index]+' '+dic[\"Colecoes\"][i][index+1:]\n",
        "              try:\n",
        "                df.to_csv(f'/content/drive/MyDrive/Programa - Repositório Institucional UFSC/Extração de Dados/via Web Scraping/Otimizações/Resultados/Coleções Repositorio UFSC/Links dos trabalhos - Repositorio UFSC/{dic[\"Colecoes\"][i]}_Links_dos_Trabalhos.csv',index=False,encoding='utf-8',sep=',')\n",
        "              except:\n",
        "                print(f'\\nErro ao fazer download do arquivo .csv para coleção {dic[\"Colecoes\"][i]}.\\nDescrição do erro --> '+str(sys.exc_info()[0].__name__)+\":\",str(sys.exc_info()[1])+'\\n')\n",
        "                lista_de_falhas['Erros'].append(f'Erro ao fazer download do arquivo .csv para coleção {dic[\"Colecoes\"][i]}.\\nDescrição do erro --> '+str(sys.exc_info()[0].__name__)+\": \"+str(sys.exc_info()[1])+'\\n')\n",
        "              else:\n",
        "                print(f'\\nDownload do arquivo .csv da coleção {dic[\"Colecoes\"][i]} feito com sucesso!\\n')\n",
        "\n",
        "\n",
        "        # # Código abaixo comentado, pois era apenas para fins de limitar o número de coleções que serão visitadas\n",
        "        # num_colecao_maxima = 4\n",
        "        # if num_colecao >= num_colecao_maxima-1:\n",
        "        #   print(f'\\nParou na {num_colecao}a coleção.')\n",
        "        #   break\n",
        "\n",
        "  print('\\n')\n",
        "  print('='*100)\n",
        "  print('Programa executado com sucesso!\\nNúmero total de links de trabalhos coletados:',numero_total_de_trabalhos)\n",
        "  print(f'Número de Coleções visitadas:',len(dic[\"Links colecoes\"]))\n",
        "  print('='*100)\n",
        "  print('\\n')\n",
        "  print('='*100)\n",
        "  print('Número de avisos gerados:',len(lista_de_falhas[\"Avisos\"]))\n",
        "  print('Número de erros gerados:',len(lista_de_falhas[\"Erros\"]))\n",
        "  print('='*100)\n",
        "\n",
        "\n",
        "  if (len(lista_de_falhas['Avisos']) > 0):\n",
        "    file_path = '/content/drive/MyDrive/Programa - Repositório Institucional UFSC/Extração de Dados/via Web Scraping/Otimizações/Resultados/Coleções Repositorio UFSC/Links dos trabalhos - Repositorio UFSC/AVISOS primeira metade.txt'\n",
        "    texto = 'AVISOS:\\n\\n'\n",
        "    for i in range(len(lista_de_falhas['Avisos'])):\n",
        "      texto = texto + '\\n' + lista_de_falhas['Avisos'][i]\n",
        "    with open(file_path, 'w') as f:\n",
        "      f.write(texto)\n",
        "      print('\\nArquivo de avisos salvo com sucesso.\\nVerifique-o!')\n",
        "      f.close()\n",
        "  if (len(lista_de_falhas['Erros']) > 0):\n",
        "    file_path = '/content/drive/MyDrive/Programa - Repositório Institucional UFSC/Extração de Dados/via Web Scraping/Otimizações/Resultados/Coleções Repositorio UFSC/Links dos trabalhos - Repositorio UFSC/ERROS primeira metade.txt'\n",
        "    texto = 'ERROS:\\n\\n'\n",
        "    for i in range(len(lista_de_falhas['Erros'])):\n",
        "      texto = texto + '\\n' + lista_de_falhas['Erros'][i]\n",
        "    with open(file_path, 'w') as f:\n",
        "      f.write(texto)\n",
        "      print('\\nArquivo de erros salvo com sucesso.\\nVerifique-o!')\n",
        "      f.close()\n",
        "\n",
        "  end = time.time()\n",
        "\n",
        "  duracao = end - start\n",
        "  print('\\n')\n",
        "  print('='*100)\n",
        "  print('Duração total da primeira execução:',round(duracao,2))\n",
        "  print('='*100)\n",
        "else:\n",
        "  print('\\nFalha na execução das células anteriores. Volte, leia as saídas das células anteriores e tente executá-las novamente.\\n')"
      ],
      "metadata": {
        "id": "8HclKfz_VP1c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}